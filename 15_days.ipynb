{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMIFaNZJfPdgqoCf/6sZMNc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karthikmandelli/frugal-labs-/blob/main/15_days.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pA4U5U_mCdrZ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "data = pd.read_csv(\"/content/SRM-DATA-FROM-JAN-1.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error,r2_score\n",
        "import numpy as np \n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from datetime import datetime, timedelta"
      ],
      "metadata": {
        "id": "yZi3hMVgIYiI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming you have a 'date' column and a 'temperature' column in your dataset\n",
        "data['Date'] = pd.to_datetime(data['Date'])\n",
        "data['AirTC'] = pd.to_numeric(data['AirTC'])\n"
      ],
      "metadata": {
        "id": "OTXzLuiiIdf6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['AirPP'] = pd.to_numeric(data['AirPP'])\n",
        "data['AirHP'] = pd.to_numeric(data['AirHP'])"
      ],
      "metadata": {
        "id": "PnXk0icVIfw7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = pd.to_datetime(data['Date']).values.reshape(-1, 1)\n",
        "y = data['AirTC'].values.reshape(-1, 1)"
      ],
      "metadata": {
        "id": "_hwcWImmIj9a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Additional Feature Engineering (example: extracting month and day of the week)\n",
        "data['month'] = data['Date'].dt.month\n",
        "data['day_of_week'] = data['Date'].dt.dayofweek\n",
        "\n",
        "X = data[['month', 'day_of_week']].values\n",
        "y = data['AirTC'].values"
      ],
      "metadata": {
        "id": "Jq0LnZIFIn0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = data[['AirPP', 'AirHP']].values\n",
        "y = data['AirTC'].values"
      ],
      "metadata": {
        "id": "uh_D73B0I5Zd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Model Training\n",
        "# Step 3: Model Training\n",
        "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "rw-apgk8I_pH",
        "outputId": "ba0b7b34-9b25-45c1-df71-6064bc1ce2ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestRegressor(random_state=42)"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestRegressor(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(random_state=42)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##############by using means grouping \n",
        "\n",
        "# Step 2: Data Preprocessing\n",
        "data['Date'] = pd.to_datetime(data['Date'])  # Convert datetime column to pandas datetime format\n",
        "data['hour'] = data['Date'].dt.hour  # Extract hour from datetime column\n",
        "data['day'] = data['Date'].dt.day  # Extract day from datetime column\n",
        "\n",
        "# Step 3: Calculate average temperature for each hour\n",
        "hourly_avg_temp = data.groupby('hour')['AirTC'].max()\n",
        "#x1 = [data.groupby('hour')['AirTC'].max()-data.groupby('hour')['AirTC'].mean()]\n",
        "# Step 4: Generate next 15 days' datetime at hourly intervals\n",
        "next_18_days = pd.date_range(start=data['Date'].max(), periods=24*18, freq='17T')\n",
        "\n",
        "# Step 5: Extract hour and day for the next 15 days\n",
        "next_hour = [dt.hour for dt in next_18_days]\n",
        "next_day = [dt.day for dt in next_18_days]\n",
        "\n",
        "# Step 6: Create DataFrame with predictions for the next 15 days\n",
        "predictions = pd.DataFrame({'datetime': next_18_days})\n",
        "\n",
        "# Step 7: Fill in temperature predictions based on average temperature at the corresponding hour\n",
        "predictions['temperature'] = [hourly_avg_temp[hour]-2.8 for hour in next_hour]\n",
        "\n",
        "print(predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lL5HZOxkJO66",
        "outputId": "17ed57a5-1380-400e-ca66-bc9e581ae08c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "               datetime  temperature\n",
            "0   2023-05-19 23:56:57        28.45\n",
            "1   2023-05-20 00:13:57        27.96\n",
            "2   2023-05-20 00:30:57        27.96\n",
            "3   2023-05-20 00:47:57        27.96\n",
            "4   2023-05-20 01:04:57        27.66\n",
            "..                  ...          ...\n",
            "427 2023-05-25 00:55:57        27.96\n",
            "428 2023-05-25 01:12:57        27.66\n",
            "429 2023-05-25 01:29:57        27.66\n",
            "430 2023-05-25 01:46:57        27.66\n",
            "431 2023-05-25 02:03:57        27.08\n",
            "\n",
            "[432 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SUBSET = predictions.iloc[135:165]"
      ],
      "metadata": {
        "id": "9Zi0teK1JSt6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(SUBSET)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n9BVS9b1JigS",
        "outputId": "667360b7-cb8a-42cd-b4af-ce2dc9d12a3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "               datetime  temperature\n",
            "135 2023-05-21 14:11:57        40.37\n",
            "136 2023-05-21 14:28:57        40.37\n",
            "137 2023-05-21 14:45:57        40.37\n",
            "138 2023-05-21 15:02:57        40.59\n",
            "139 2023-05-21 15:19:57        40.59\n",
            "140 2023-05-21 15:36:57        40.59\n",
            "141 2023-05-21 15:53:57        40.59\n",
            "142 2023-05-21 16:10:57        40.83\n",
            "143 2023-05-21 16:27:57        40.83\n",
            "144 2023-05-21 16:44:57        40.83\n",
            "145 2023-05-21 17:01:57        39.73\n",
            "146 2023-05-21 17:18:57        39.73\n",
            "147 2023-05-21 17:35:57        39.73\n",
            "148 2023-05-21 17:52:57        39.73\n",
            "149 2023-05-21 18:09:57        32.50\n",
            "150 2023-05-21 18:26:57        32.50\n",
            "151 2023-05-21 18:43:57        32.50\n",
            "152 2023-05-21 19:00:57        31.64\n",
            "153 2023-05-21 19:17:57        31.64\n",
            "154 2023-05-21 19:34:57        31.64\n",
            "155 2023-05-21 19:51:57        31.64\n",
            "156 2023-05-21 20:08:57        29.75\n",
            "157 2023-05-21 20:25:57        29.75\n",
            "158 2023-05-21 20:42:57        29.75\n",
            "159 2023-05-21 20:59:57        29.75\n",
            "160 2023-05-21 21:16:57        29.55\n",
            "161 2023-05-21 21:33:57        29.55\n",
            "162 2023-05-21 21:50:57        29.55\n",
            "163 2023-05-21 22:07:57        28.97\n",
            "164 2023-05-21 22:24:57        28.97\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "\n",
        "# Assume the dataset contains columns 'Date' and 'Temperature' with daily temperature values for each May\n",
        "\n",
        "# Step 2: Extract Features\n",
        "data['Date'] = pd.to_datetime(data['Date'])  # Convert Date column to pandas datetime format\n",
        "data['Day'] = data['Date'].dt.day  # Extract day from Date column\n",
        "data['Temperature'] = data['AirTC'].astype(float)  # Convert Temperature column to float\n",
        "\n",
        "# Step 3: Split Data\n",
        "train_data = data[:-365]  # Use data from previous years as the training set\n",
        "test_data = data[-365:]  # Keep the most recent year as the testing set\n",
        "\n",
        "# Step 4: Create a SARIMA Model\n",
        "model = SARIMAX(train_data['Temperature'], order=(1, 0, 0), seasonal_order=(1, 1, 1, 12))\n",
        "\n",
        "# Step 5: Train the Model\n",
        "model_fit = model.fit()\n",
        "\n",
        "# Step 6: Evaluate the Model\n",
        "predictions = model_fit.predict(start=len(train_data), end=len(train_data) + len(test_data) - 1)\n",
        "\n",
        "# Step 7: Make Predictions for May of the Current Year\n",
        "current_year_data = pd.DataFrame({'Day': range(1, 32)})  # Assuming May has 31 days\n",
        "forecast = model_fit.predict(start=len(train_data), end=len(train_data) + len(current_year_data) - 1)\n",
        "\n",
        "# Print the forecasted temperatures for May of the current year\n",
        "print(forecast)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0QXqQyBcQNi",
        "outputId": "005589d4-d62a-4f9f-8b1b-b88fa60aca51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10776    24.369287\n",
            "10777    24.404844\n",
            "10778    24.492291\n",
            "10779    24.595803\n",
            "10780    24.722269\n",
            "10781    24.770942\n",
            "10782    24.850048\n",
            "10783    24.943133\n",
            "10784    25.070573\n",
            "10785    25.125999\n",
            "10786    25.231383\n",
            "10787    25.321383\n",
            "10788    25.360277\n",
            "10789    25.316916\n",
            "10790    25.336917\n",
            "10791    25.379328\n",
            "10792    25.441510\n",
            "10793    25.435307\n",
            "10794    25.464824\n",
            "10795    25.506735\n",
            "10796    25.595450\n",
            "10797    25.613003\n",
            "10798    25.674792\n",
            "10799    25.732606\n",
            "10800    25.739227\n",
            "10801    25.666004\n",
            "10802    25.658635\n",
            "10803    25.675858\n",
            "10804    25.714623\n",
            "10805    25.686963\n",
            "10806    25.696737\n",
            "Name: predicted_mean, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "tAgx_JYpiHBE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85Qo5z9fvUjN",
        "outputId": "1b86e796-70a6-4320-818e-cfe13f562f96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (2.12.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import LSTM\n"
      ],
      "metadata": {
        "id": "DNea91qnvaXL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from matplotlib import rcParams\n",
        "rcParams['figure.figsize']=10,6\n",
        "from keras.layers.core import Dense, Activation, Dropout\n",
        "from keras.models import Sequential\n",
        "import time\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from matplotlib import pyplot\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "id": "zZDS9YnUfyO7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "# Step 1: Data Collection\n",
        "data = pd.read_csv('/content/SRM-DATA-FROM-JAN-1.csv')  # Assuming bengaluru_rainfall_data.csv contains date and rainfall columns\n",
        "\n",
        "# Step 2: Data Preprocessing\n",
        "data['Date'] = pd.to_datetime(data['Date'])  # Convert Date column to pandas datetime format\n",
        "data['Year'] = data['Date'].dt.year\n",
        "data['Month'] = data['Date'].dt.month\n",
        "data['Day'] = data['Date'].dt.day\n",
        "\n",
        "# Step 3: Feature Selection\n",
        "features = ['Year', 'Month', 'Day']  # Adjust the features based on your dataset and domain knowledge\n",
        "\n",
        "# Step 4: Splitting the Data\n",
        "X = data[features]\n",
        "y = data['RF']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 5: Selecting a Model\n",
        "model = RandomForestRegressor()\n",
        "\n",
        "# Step 6: Training the Model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 7: Model Evaluation\n",
        "predictions = model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, predictions)\n",
        "mae = mean_absolute_error(y_test, predictions)\n",
        "\n",
        "print('Mean Squared Error:', mse)\n",
        "print('Mean Absolute Error:', mae)\n",
        "\n",
        "# Step 8: Making Predictions\n",
        "\n",
        "\n",
        "future_predictions = model.predict(future_data)\n",
        "\n",
        "print('Future Rainfall Predictions:')\n",
        "print(future_predictions)\n"
      ],
      "metadata": {
        "id": "PbDlqMrxv0KS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "outputId": "e9f78b26-beb0-4851-ef05-93b8c5830e30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 364.02737978324717\n",
            "Mean Absolute Error: 0.41138588171985574\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-f3c3149c3a1a>\u001b[0m in \u001b[0;36m<cell line: 40>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0mfuture_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfuture_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Future Rainfall Predictions:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'future_data' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression\n",
        "# Step 2: Data Preprocessing\n",
        "data['Date'] = pd.to_datetime(data['Date'])  # Convert date column to pandas datetime format\n",
        "data['Day'] = data['Date'].dt.day  # Extract day from the date column\n",
        "data['Month'] = data['Date'].dt.month  # Extract month from the date column\n",
        "# Step 3: Feature Engineering\n",
        "X = data[['Day', 'Month']]  # Input features (day and month)\n",
        "y = data['RF']  # Target variable (rainfall)\n",
        "# Step 4: Splitting the Data\n",
        "X_train = X[:-15]  # Training data (all data except the last 15 days)\n",
        "y_train = y[:-15]\n",
        "X_test = X[-15:]  # Testing data (last 15 days)\n",
        "y_test = y[-15:]\n",
        "model = LinearRegression()\n",
        "# Step 6: Training the Model\n",
        "model.fit(X_train, y_train)\n",
        "# Step 7: Model Evaluation\n",
        "accuracy = model.score(X_test, y_test)\n",
        "print('Model Accuracy:', accuracy)\n",
        "# Step 8: Making Predictions for the Next 15 Days\n",
        "next_15_days = pd.date_range(start=data['Date'].max() + pd.Timedelta(days=1), periods=24, freq='D')\n",
        "next_days = next_15_days.day\n",
        "next_months = next_15_days.month\n",
        "new_data = pd.DataFrame({'Day': next_days, 'Month': next_months})\n",
        "predictions = model.predict(new_data)\n",
        "print('Next 15 days rainfall predictions:')\n",
        "print(predictions)\n"
      ],
      "metadata": {
        "id": "qBmFmRf7X-tl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression\n",
        " # Assuming rainfall_data.csv contains date and rainfall columns\n",
        "\n",
        "# Step 2: Data Preprocessing\n",
        "data['Date'] = pd.to_datetime(data['Date'])  # Convert Date column to pandas datetime format\n",
        "data['day'] = data['Date'].dt.day  # Extract day from Date column\n",
        "data['month'] = data['Date'].dt.month  # Extract month from Date column\n",
        "\n",
        "# Step 3: Feature Engineering\n",
        "X = data[['day', 'month']]  # Input features: day and month\n",
        "y = data['RF']  # Target variable: rainfall\n",
        "\n",
        "# Step 4: Splitting the Data\n",
        "X_train = X[:-15]  # Use all data except the last 15 days for training\n",
        "y_train = y[:-15]\n",
        "X_test = X[-15:]  # Use the last 15 days for testing\n",
        "y_test = y[-15:]\n",
        "\n",
        "# Step 5: Selecting a Model\n",
        "model = LinearRegression()\n",
        "\n",
        "# Step 6: Training the Model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 7: Model Evaluation\n",
        "accuracy = model.score(X_test,predictions)\n",
        "print('Model Accuracy:', -accuracy)\n",
        "\n",
        "# Step 8: Making Predictions for Next 15 Days\n",
        "next_15_days = pd.DataFrame()\n",
        "next_15_days['Date'] = pd.date_range(start=data['Date'].max() + pd.DateOffset(days=1), periods=15, freq='D')  # Generate next 15 days' dates\n",
        "next_15_days['day'] = next_15_days['Date'].dt.day  # Extract day from Date column\n",
        "next_15_days['month'] = next_15_days['Date'].dt.month  # Extract month from Date column\n",
        "\n",
        "predictions = model.predict(next_15_days[['day', 'month']])\n",
        "\n",
        "# Combine dates and corresponding rainfall predictions\n",
        "predictions_df = pd.DataFrame({'Date': next_15_days['Date'], 'Rainfall Prediction': predictions})\n",
        "\n",
        "print('Rainfall Predictions for the Next 15 Days:')\n",
        "print(predictions_df)\n"
      ],
      "metadata": {
        "id": "OPGc5luYajji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import cross_val_score\n",
        "# Step 2: Data Preprocessing\n",
        "data['Date'] = pd.to_datetime(data['Date'])  # Convert Date column to pandas datetime format\n",
        "data['day'] = data['Date'].dt.day  # Extract day from Date column\n",
        "data['month'] = data['Date'].dt.month  # Extract month from Date column\n",
        "# Step 3: Feature Engineering\n",
        "X = data[['day', 'month']]  # Input features: day and month\n",
        "y = data['RF']  # Target variable: rainfall\n",
        "# Step 4: Model Selection and Cross-Validation\n",
        "model = LinearRegression()\n",
        "scores = cross_val_score(model, X, y, cv=5)  # Perform 5-fold cross-validation\n",
        "# Print cross-validation scores\n",
        "print('Cross-Validation Scores:', scores)\n",
        "print('Average Cross-Validation Score:', scores.mean())\n",
        "# Step 5: Training the Model\n",
        "model.fit(X, y)\n",
        "# Step 6: Making Predictions for Next 15 Days\n",
        "next_15_days = pd.DataFrame()\n",
        "next_15_days['Date'] = pd.date_range(start=data['Date'].max() + pd.DateOffset(days=1), periods=15, freq='D')  # Generate next 15 days' dates\n",
        "next_15_days['day'] = next_15_days['Date'].dt.day  # Extract day from Date column\n",
        "next_15_days['month'] = next_15_days['Date'].dt.month  # Extract month from Date column\n",
        "predictions = model.predict(next_15_days[['day', 'month']])\n",
        "# Combine dates and corresponding rainfall predictions\n",
        "predictions_df = pd.DataFrame({'Date': next_15_days['Date'], 'Rainfall Prediction': predictions})\n",
        "print('Rainfall Predictions for the Next 15 Days:')\n",
        "print(predictions_df)\n",
        "accuracy = model.score(X_test,predictions)\n",
        "print('Model Accuracy:', accuracy)\n"
      ],
      "metadata": {
        "id": "l9aJkU0NdWlu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import cross_val_score\n",
        "# Step 2: Data Preprocessing\n",
        "data['Date'] = pd.to_datetime(data['Date'])  # Convert Date column to pandas datetime format\n",
        "data['day'] = data['Date'].dt.day  # Extract day from Date column\n",
        "data['month'] = data['Date'].dt.month  # Extract month from Date column\n",
        "# Step 3: Feature Engineering\n",
        "X = data[['day', 'month']]  # Input features: day and month\n",
        "y = data['RF']  # Target variable: rainfall\n",
        "# Step 4: Model Selection and Cross-Validation\n",
        "model = RandomForestRegressor()\n",
        "scores = cross_val_score(model, X, y, cv=5)  # Perform 5-fold cross-validation\n",
        "# Print cross-validation scores\n",
        "print('Cross-Validation Scores:', scores)\n",
        "print('Average Cross-Validation Score:', scores.mean())\n",
        "# Step 5: Training the Model\n",
        "model.fit(X, y)\n",
        "# Step 6: Making Predictions for Next 15 Days\n",
        "next_15_days = pd.DataFrame()\n",
        "next_15_days['Date'] = pd.date_range(start=data['Date'].max() + pd.DateOffset(days=1), periods=15, freq='D')  # Generate next 15 days' dates\n",
        "next_15_days['day'] = next_15_days['Date'].dt.day  # Extract day from Date column\n",
        "next_15_days['month'] = next_15_days['Date'].dt.month  # Extract month from Date column\n",
        "predictions = model.predict(next_15_days[['day', 'month']])\n",
        "# Combine dates and corresponding rainfall predictions\n",
        "predictions_df = pd.DataFrame({'Date': next_15_days['Date'], 'Rainfall Prediction': predictions})\n",
        "print('Rainfall Predictions for the Next 15 Days:')\n",
        "print(predictions_df)\n",
        "accuracy = model.score(X_test,predictions)\n",
        "print('Model Accuracy:', accuracy)\n",
        "\n"
      ],
      "metadata": {
        "id": "Y6Z1eryXfL38"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import cross_val_score\n",
        "# Step 2: Data Preprocessing\n",
        "data['Date'] = pd.to_datetime(data['Date'])  # Convert Date column to pandas datetime format\n",
        "data['day'] = data['Date'].dt.day  # Extract day from Date column\n",
        "data['month'] = data['Date'].dt.month  # Extract month from Date column\n",
        "# Step 3: Feature Engineering\n",
        "X = data[['day', 'month', 'AirPP', 'AirTC', 'AirHP']]  # Input features: day, month, pressure, temperature, humidity\n",
        "y = data['RF']  # Target variable: rainfall\n",
        "# Step 4: Model Selection and Cross-Validation\n",
        "model = RandomForestRegressor()\n",
        "scores = cross_val_score(model, X, y, cv=5)  # Perform 5-fold cross-validation\n",
        "# Print cross-validation scores\n",
        "print('Cross-Validation Scores:', scores)\n",
        "print('Average Cross-Validation Score:', scores.mean())\n",
        "# Step 5: Training the Model\n",
        "model.fit(X, y)\n",
        "# Step 6: Making Predictions for Next 15 Days\n",
        "next_15_days = pd.DataFrame()\n",
        "next_15_days['Date'] = pd.date_range(start=data['Date'].max() + pd.DateOffset(days=1), periods=15, freq='D')  # Generate next 15 days' dates\n",
        "next_15_days['day'] = next_15_days['Date'].dt.day  # Extract day from Date column\n",
        "next_15_days['month'] = next_15_days['Date'].dt.month  # Extract month from Date column\n",
        "\n",
        "# Use the latest available pressure, temperature, and humidity values for predictions\n",
        "latest_pressure = data['AirPP'].iloc[-1]\n",
        "latest_temperature = data['AirTC'].iloc[-1]\n",
        "latest_humidity = data['AirHP'].iloc[-1]\n",
        "\n",
        "next_15_days['AirPP'] = latest_pressure\n",
        "next_15_days['AirTC'] = latest_temperature\n",
        "next_15_days['AirHP'] = latest_humidity\n",
        "predictions = model.predict(next_15_days[['day', 'month', 'AirPP', 'AirTC', 'AirHP']])\n",
        "# Combine dates and corresponding rainfall predictions\n",
        "predictions_df = pd.DataFrame({'Date': next_15_days['Date'], 'Rainfall Prediction': predictions})\n",
        "print('Rainfall Predictions for the Next 15 Days:')\n",
        "print(predictions_df)\n",
        "print('Cross-Validation Scores:', scores)\n",
        "print('Average Cross-Validation Score:', scores.mean())\n"
      ],
      "metadata": {
        "id": "Zd0QF305gnQL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bEncutnQjaTg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow"
      ],
      "metadata": {
        "id": "Ku6guxxvt6vF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras "
      ],
      "metadata": {
        "id": "B-gI7I9xuwNn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "#suming weather_data.csv contains date, pressure, temperature, humidity, and rainfall columns\n",
        "\n",
        "# Step 2: Data Preprocessing\n",
        "data['Date'] = pd.to_datetime(data['Date'])  # Convert Date column to pandas datetime format\n",
        "data['day'] = data['Date'].dt.day  # Extract day from Date column\n",
        "data['month'] = data['Date'].dt.month  # Extract month from Date column\n",
        "\n",
        "# Step 3: Feature Engineering\n",
        "features = ['day', 'month', 'AirPP', 'AirTC', 'AirHP']\n",
        "target = 'RF'\n",
        "\n",
        "# Scale the features and target variable\n",
        "scaler = MinMaxScaler()\n",
        "data[features + [target]] = scaler.fit_transform(data[features + [target]])\n",
        "\n",
        "# Step 4: Prepare the Data for LSTM\n",
        "def create_lstm_dataset(X, y, time_steps=1):\n",
        "    Xs, ys = [], []\n",
        "    for i in range(len(X) - time_steps):\n",
        "        Xs.append(X.iloc[i:i + time_steps].values)\n",
        "        ys.append(y.iloc[i + time_steps])\n",
        "    return np.array(Xs), np.array(ys)\n",
        "\n",
        "time_steps = 10  # Number of previous time steps to consider\n",
        "X, y = create_lstm_dataset(data[features], data[target], time_steps)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "split_ratio = 0.8  # 80% for training, 20% for testing\n",
        "split_index = int(split_ratio * len(X))\n",
        "X_train, X_test = X[:split_index], X[split_index:]\n",
        "y_train, y_test = y[:split_index], y[split_index:]\n",
        "\n",
        "# Step 5: Build and Train the LSTM Model\n",
        "model = Sequential()\n",
        "model.add(LSTM(units=64, activation='relu', input_shape=(time_steps, len(features))))\n",
        "model.add(Dense(units=1))\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32)\n",
        "\n",
        "# Step 6: Prediction and Evaluation\n",
        "y_train_pred = model.predict(X_train)\n",
        "y_test_pred = model.predict(X_test)\n",
        "\n",
        "mse_train = mean_squared_error(y_train, y_train_pred)\n",
        "mse_test = mean_squared_error(y_test, y_test_pred)\n",
        "\n",
        "print('Train Mean Squared Error:', mse_train)\n",
        "print('Test Mean Squared Error:', mse_test)\n",
        "\n",
        "# Step 7: Making Predictions for Next 15 Days\n",
        "last_sequence = data[features].tail(time_steps).values\n",
        "next_15_days = pd.DataFrame(last_sequence, columns=features)\n",
        "\n",
        "next_15_days_pred = []\n",
        "for _ in range(15):\n",
        "    next_pred = model.predict(np.expand_dims(next_15_days[-time_steps:].values, axis=0))\n",
        "    next_15_days_pred.append(next_pred[0, 0])\n",
        "    next_15_days = next_15_days.append({**dict(zip(features, next_pred[0]))}, ignore_index=True)\n",
        "\n",
        "next_15_days[target] = scaler.inverse_transform(np.array(next_15_days_pred).reshape(-1, 1))\n",
        "\n",
        "print('Rainfall Predictions for the Next 15 Days:')\n",
        "print(next_15_days[['day', 'month', target]])\n"
      ],
      "metadata": {
        "id": "yYTGcLN9twhP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Step 1: Data Collection\n",
        "data = pd.read_csv('weather_data.csv')  # Assuming weather_data.csv contains date, pressure, temperature, humidity, and rainfall columns\n",
        "\n",
        "# Step 2: Data Preprocessing\n",
        "data['Date'] = pd.to_datetime(data['Date'])  # Convert Date column to pandas datetime format\n",
        "data['day'] = data['Date'].dt.day  # Extract day from Date column\n",
        "data['month'] = data['Date'].dt.month  # Extract month from Date column\n",
        "\n",
        "# Step 3: Feature Engineering\n",
        "features = ['day', 'month', 'pressure', 'temperature', 'humidity']\n",
        "target = 'rainfall'\n",
        "\n",
        "# Scale the features and target variable\n",
        "scaler = MinMaxScaler()\n",
        "data[features + [target]] = scaler.fit_transform(data[features + [target]])\n",
        "\n",
        "# Step 4: Prepare the Data for LSTM\n",
        "def create_lstm_dataset(X, y, time_steps=1):\n",
        "    Xs, ys = [], []\n",
        "    for i in range(len(X) - time_steps):\n",
        "        Xs.append(X.iloc[i:i + time_steps].values)\n",
        "        ys.append(y.iloc[i + time_steps])\n",
        "    return np.array(Xs), np.array(ys)\n",
        "\n",
        "time_steps = 10  # Number of previous time steps to consider\n",
        "X, y = create_lstm_dataset(data[features], data[target], time_steps)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "split_ratio = 0.8  # 80% for training, 20% for testing\n",
        "split_index = int(split_ratio * len(X))\n",
        "X_train, X_test = X[:split_index], X[split_index:]\n",
        "y_train, y_test = y[:split_index], y[split_index:]\n",
        "\n",
        "# Step 5: Build and Train the LSTM Model\n",
        "model = Sequential()\n",
        "model.add(LSTM(units=64, activation='relu', input_shape=(time_steps, len(features))))\n",
        "model.add(Dense(units=1))\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32)\n",
        "\n",
        "# Step 6: Prediction and Evaluation\n",
        "y_train_pred = model.predict(X_train)\n",
        "y_test_pred = model.predict(X_test)\n",
        "\n",
        "mse_train = mean_squared_error(y_train, y_train_pred)\n",
        "mse_test = mean_squared_error(y_test, y_test_pred)\n",
        "\n",
        "print('Train Mean Squared Error:', mse_train)\n",
        "print('Test Mean Squared Error:', mse_test)\n",
        "\n",
        "# Step 7: Making Predictions for Next 15 Days\n",
        "last_sequence = data[features].tail(time_steps).values\n",
        "next_15_days = pd.DataFrame(last_sequence, columns=features)\n",
        "\n",
        "next_15_days_pred = []\n",
        "for _ in range(15):\n",
        "    next_pred = model.predict(np.expand_dims(next_15_days[-time_steps:].values, axis=0))\n",
        "    next_15_days_pred.append(next_pred[0, 0])\n",
        "    next_15_days = next_15_days.append({**dict(zip(features, next_pred[0]))}, ignore_index=True)\n",
        "\n",
        "next_15_days[target] = scaler.inverse_transform(np.array(next_15_days_pred).reshape(-1, 1))\n",
        "\n",
        "print('Rainfall Predictions for the Next 15 Days:')\n",
        "print(next_15_days[['day', 'month', target]])\n"
      ],
      "metadata": {
        "id": "8MFLkv9APVUp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#########kaggle"
      ],
      "metadata": {
        "id": "X0s4XhmAXTjE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "data = pd.read_csv(\"/content/rainfall in india 1901-2015.csv\")"
      ],
      "metadata": {
        "id": "URqinD5TXScr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head(75)"
      ],
      "metadata": {
        "id": "c5NB6FNCXpPc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape"
      ],
      "metadata": {
        "id": "K8IhAvCcXvbF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.columns"
      ],
      "metadata": {
        "id": "YS6Ko1ykYFRk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.info()"
      ],
      "metadata": {
        "id": "LLwSmx9iYHbQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.isnull().sum()"
      ],
      "metadata": {
        "id": "1sHy4mJCYK8T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.dropna()"
      ],
      "metadata": {
        "id": "VUbZBtCCYStw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.isnull().sum()"
      ],
      "metadata": {
        "id": "rP7ocpjUYWeq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape"
      ],
      "metadata": {
        "id": "rXvF0eojZQSn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.columns"
      ],
      "metadata": {
        "id": "SPHFwF1BZhA9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img=np.array(Image.open('../input/indian-rainfall/IndiaRainfall.png'))\n",
        "fig=plt.figure(figsize=(10,10))\n",
        "plt.imshow(img,interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.ioff()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7xC4QKwXaiB5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "  # Replace 'historical_data.csv' with your file path or data source\n",
        "df = pd.read_csv(\"/content/SRM-DATA-FROM-JAN-1.csv\")\n",
        "# Convert the date column to datetime type\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "\n",
        "# Extract year, month, day, and hour from the Date column\n",
        "df['Year'] = df['Date'].dt.year\n",
        "df['Month'] = df['Date'].dt.month\n",
        "df['Day'] = df['Date'].dt.day\n",
        "df['Hour'] = df['Date'].dt.hour\n",
        "\n",
        "# Group the data by date and hour\n",
        "grouped_data = df.groupby(['Year', 'Month', 'Day', 'Hour'])\n",
        "\n",
        "# Calculate average temperature for each group\n",
        "average_temperatures = grouped_data['AirTC'].mean()\n",
        "\n",
        "# Iterate over the average temperatures and extract the desired time slots\n",
        "predicted_temperatures = []\n",
        "for i, temp in average_temperatures.iteritems():\n",
        "    year, month, day, hour = i\n",
        "    if hour in [1, 2, 3, 4, 5, 6]:\n",
        "        time_slot = '1-6 AM'\n",
        "    elif hour in [7, 8, 9, 10, 11, 12]:\n",
        "        time_slot = '6-12 PM'\n",
        "    elif hour in [13, 14, 15, 16, 17, 18]:\n",
        "        time_slot = '12-6 PM'\n",
        "    else:\n",
        "        time_slot = '6-12 AM'\n",
        "    predicted_temperatures.append([year, month, day, time_slot, temp])\n",
        "\n",
        "# Create a new DataFrame with the predicted temperatures\n",
        "predicted_df = pd.DataFrame(predicted_temperatures, columns=['Year', 'Month', 'Day', 'Time Slot', 'Temperature'])\n",
        "\n",
        "# Print the predicted temperatures\n",
        "print(predicted_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s4oNoFWA47Zx",
        "outputId": "5f1873c7-04b4-4db8-dcd6-0faef4ae4056"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Year  Month  Day Time Slot  Temperature\n",
            "0     2022     12   13   6-12 PM    29.700000\n",
            "1     2022     12   13   6-12 PM    30.240000\n",
            "2     2022     12   13   6-12 PM    29.696667\n",
            "3     2022     12   13   12-6 PM    29.656667\n",
            "4     2022     12   13   12-6 PM    30.203333\n",
            "...    ...    ...  ...       ...          ...\n",
            "3439  2023      5   19   6-12 AM    30.655000\n",
            "3440  2023      5   19   6-12 AM    30.290000\n",
            "3441  2023      5   19   6-12 AM    30.077500\n",
            "3442  2023      5   19   6-12 AM    29.846667\n",
            "3443  2023      5   19   6-12 AM    29.775000\n",
            "\n",
            "[3444 rows x 5 columns]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-33-a8e947c8daa4>:22: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
            "  for i, temp in average_temperatures.iteritems():\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "\n",
        "#eplace 'historical_data.csv' with your file path or data source\n",
        "\n",
        "# Convert the date column to datetime type\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "\n",
        "# Set the date column as the index\n",
        "df.set_index('Date', inplace=True)\n",
        "\n",
        "# Create a SARIMA model\n",
        "model = sm.tsa.SARIMAX(df['AirTC'], order=(1, 1, 1), seasonal_order=(1, 1, 1, 12))\n",
        "\n",
        "# Fit the model to the data\n",
        "model_fit = model.fit()\n",
        "\n",
        "# Forecast the next 15 days\n",
        "forecast = model_fit.forecast(steps=15)\n",
        "\n",
        "# Print the forecasted temperatures\n",
        "print(forecast)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 523
        },
        "id": "jeUhP-xI5s8Y",
        "outputId": "1f706e89-2242-46f8-9b5b-625fb6bd379c"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3802\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3803\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Date'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-7f9208c07d72>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Convert the date column to datetime type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Set the date column as the index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3805\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3806\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3807\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3808\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3809\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3803\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3804\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3805\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3806\u001b[0m                 \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Date'"
          ]
        }
      ]
    }
  ]
}